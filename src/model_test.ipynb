{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "from itertools import repeat\n",
    "from torch_geometric.data import DataLoader, InMemoryDataset, download_url, extract_zip\n",
    "from torch_geometric.io import read_tu_data\n",
    "from torch_geometric.utils import degree\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import GINConv, global_add_pool, global_mean_pool, global_max_pool\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from dataset import TUDataset_aug, OGBDataset_aug\n",
    "from utils import setup_seed, evaluate_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "name_list = [\"hiv\", \"bbbp\", \"clintox\", \"tox21\", \"sider\"]\n",
    "i = 0\n",
    "dataset = PygGraphPropPredDataset(name = \"ogbg-mol\" + name_list[i], root = '../data/')\n",
    " \n",
    "split_idx = dataset.get_idx_split() \n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisenEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_features, emb_dim, num_layer, K, head_layers, if_proj_head=False, drop_ratio=0.0,\n",
    "                 graph_pooling='add', JK='last', residual=False, device=None, args=None):\n",
    "        super(DisenEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.num_features = num_features\n",
    "        self.K = K\n",
    "        self.d = emb_dim // self.K\n",
    "        self.num_layer = num_layer\n",
    "        self.head_layers = head_layers\n",
    "        self.gc_layers = self.num_layer - self.head_layers\n",
    "        self.if_proj_head = if_proj_head\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.graph_pooling = graph_pooling\n",
    "        if self.graph_pooling == \"sum\" or self.graph_pooling == 'add':\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "        self.JK = JK\n",
    "        if JK == 'last':\n",
    "            pass\n",
    "        elif JK == 'sum':\n",
    "            self.JK_proj = Linear(self.gc_layers * emb_dim, emb_dim)\n",
    "        else:\n",
    "            assert False\n",
    "        self.residual = residual\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.disen_convs = torch.nn.ModuleList()\n",
    "        self.disen_bns = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(self.gc_layers):\n",
    "            if i == 0:\n",
    "                nn = Sequential(Linear(num_features, emb_dim), ReLU(), Linear(emb_dim, emb_dim))\n",
    "            else:\n",
    "                nn = Sequential(Linear(emb_dim, emb_dim), ReLU(), Linear(emb_dim, emb_dim))\n",
    "            conv = GINConv(nn)\n",
    "            bn = torch.nn.BatchNorm1d(emb_dim)\n",
    "\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(bn)\n",
    "\n",
    "        for i in range(self.K):\n",
    "            for j in range(self.head_layers):\n",
    "                if j == 0:\n",
    "                    nn = Sequential(Linear(emb_dim, self.d), ReLU(), Linear(self.d, self.d))\n",
    "                else:\n",
    "                    nn = Sequential(Linear(self.d, self.d), ReLU(), Linear(self.d, self.d))\n",
    "                conv = GINConv(nn)\n",
    "                bn = torch.nn.BatchNorm1d(self.d)\n",
    "\n",
    "                self.disen_convs.append(conv)\n",
    "                self.disen_bns.append(bn)\n",
    "\n",
    "        self.proj_heads = torch.nn.ModuleList()\n",
    "        for i in range(self.K):\n",
    "            nn = Sequential(Linear(self.d, self.d), ReLU(inplace=True), Linear(self.d, self.d))\n",
    "            self.proj_heads.append(nn)\n",
    "\n",
    "    def _normal_conv(self, x, edge_index, batch):\n",
    "        xs = []\n",
    "        for i in range(self.gc_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            if i == self.gc_layers - 1:\n",
    "                x = F.dropout(x, self.drop_ratio, training=self.training)\n",
    "            else:\n",
    "                x = F.dropout(F.relu(x), self.drop_ratio, training=self.training)\n",
    "\n",
    "            if self.residual and i > 0:\n",
    "                x += xs[i - 1]\n",
    "            xs.append(x)\n",
    "        if self.JK == 'last':\n",
    "            return xs[-1]\n",
    "        elif self.JK == 'sum':\n",
    "            return self.JK_proj(torch.cat(xs, dim=-1))\n",
    "\n",
    "    def _disen_conv(self, x, edge_index, batch):\n",
    "        x_proj_list = []\n",
    "        x_proj_pool_list = []\n",
    "        for i in range(self.K):\n",
    "            x_proj = x\n",
    "            for j in range(self.head_layers):\n",
    "                tmp_index = i * self.head_layers + j\n",
    "                x_proj = self.disen_convs[tmp_index](x_proj, edge_index)\n",
    "                x_proj = self.disen_bns[tmp_index](x_proj)\n",
    "                if j != self.head_layers - 1:\n",
    "                    x_proj = F.relu(x_proj)\n",
    "            x_proj_list.append(x_proj)\n",
    "            x_proj_pool_list.append(self.pool(x_proj, batch))\n",
    "        if self.if_proj_head:\n",
    "            x_proj_pool_list = self._proj_head(x_proj_pool_list)\n",
    "        x_graph_multi = torch.stack(x_proj_pool_list)\n",
    "        x_node_multi = torch.stack(x_proj_list)\n",
    "        x_graph_multi = x_graph_multi.permute(1, 0, 2).contiguous()\n",
    "        x_node_multi = x_node_multi.permute(1, 0, 2).contiguous()\n",
    "        return x_graph_multi, x_node_multi\n",
    "\n",
    "    def _proj_head(self, x_proj_pool_list):\n",
    "        ret = []\n",
    "        for k in range(self.K):\n",
    "            x_graph_proj = self.proj_heads[k](x_proj_pool_list[k])\n",
    "            ret.append(x_graph_proj)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, x, edge_index, batch, device=\"mps\"):\n",
    "        if x is None:\n",
    "            x = torch.ones((batch.shape[0], 1)).to(device)\n",
    "        h_node = self._normal_conv(x, edge_index, batch)\n",
    "        h_graph_multi, h_node_multi = self._disen_conv(h_node, edge_index, batch)\n",
    "        return h_graph_multi, h_node_multi\n",
    "\n",
    "    def get_embeddings(self, loader):\n",
    "        device = self.device\n",
    "        ret = []\n",
    "        y = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data[0]\n",
    "                data.to(device)\n",
    "                x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "                if x is None:\n",
    "                    x = torch.ones((batch.shape[0], 1)).to(device)\n",
    "                x, _ = self.forward(x, edge_index, batch)\n",
    "                B, K, d = x.size()\n",
    "                x = x.view(B, K * d)\n",
    "                ret.append(x.cpu().numpy())\n",
    "                y.append(data.y.cpu().numpy())\n",
    "        ret = np.concatenate(ret, 0)\n",
    "        y = np.concatenate(y, 0)\n",
    "        return ret, y\n",
    "\n",
    "class DGCL(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_layer, device, args):\n",
    "        super(DGCL, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.T = self.args.tau\n",
    "        self.K = args.num_latent_factors\n",
    "        self.embedding_dim = hidden_dim\n",
    "        self.d = self.embedding_dim // self.K\n",
    "\n",
    "        self.center_v = torch.rand((self.K, self.d), requires_grad=True).to(device)\n",
    "\n",
    "        self.encoder = DisenEncoder(\n",
    "            num_features=num_features,\n",
    "            emb_dim=hidden_dim,\n",
    "            num_layer=num_layer,\n",
    "            K=args.num_latent_factors,\n",
    "            head_layers=args.head_layers,\n",
    "            device=device,\n",
    "            args=args,\n",
    "            if_proj_head=args.proj > 0,\n",
    "            drop_ratio=args.drop_ratio,\n",
    "            graph_pooling=args.pool,\n",
    "            JK=args.JK,\n",
    "            residual=args.residual > 0\n",
    "        )\n",
    "\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = -1.5 / self.embedding_dim\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, num_graphs):\n",
    "        if x is None:\n",
    "            x = torch.ones(batch.shape[0]).to(device)\n",
    "        z_graph, _ = self.encoder(x, edge_index, batch)\n",
    "        return z_graph\n",
    "\n",
    "    def loss_cal(self, x, x_aug):\n",
    "        T = self.T\n",
    "        T_c = 0.2\n",
    "        B, H, d = x.size()\n",
    "        ck = F.normalize(self.center_v)\n",
    "        p_k_x_ = torch.einsum('bkd,kd->bk', F.normalize(x, dim=-1), ck)\n",
    "        p_k_x = F.softmax(p_k_x_ / T_c, dim=-1)\n",
    "        x_abs = x.norm(dim=-1)\n",
    "        x_aug_abs = x_aug.norm(dim=-1)\n",
    "        x = torch.reshape(x, (B * H, d))\n",
    "        x_aug = torch.reshape(x_aug, (B * H, d))\n",
    "        x_abs = torch.squeeze(torch.reshape(x_abs, (B * H, 1)), 1)\n",
    "        x_aug_abs = torch.squeeze(torch.reshape(x_aug_abs, (B * H, 1)), 1)\n",
    "        sim_matrix = torch.einsum('ik,jk->ij', x, x_aug) / (1e-8 + torch.einsum('i,j->ij', x_abs, x_aug_abs))\n",
    "        sim_matrix = torch.exp(sim_matrix / T)\n",
    "        pos_sim = sim_matrix[range(B * H), range(B * H)]\n",
    "        score = pos_sim / (sim_matrix.sum(dim=-1) - pos_sim)\n",
    "        p_y_xk = score.view(B, H)\n",
    "        q_k = torch.einsum('bk,bk->bk', p_k_x, p_y_xk)\n",
    "        q_k = F.normalize(q_k, dim=-1)\n",
    "        elbo = q_k * (torch.log(p_k_x) + torch.log(p_y_xk) - torch.log(q_k))\n",
    "        loss = - elbo.view(-1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse(DS=\"MUTAG\"):\n",
    "    parser = argparse.ArgumentParser(description='DGCL Arguments.')\n",
    "\n",
    "    parser.add_argument('--DS', type=str, default=DS)\n",
    "    parser.add_argument('--lr', type=float)\n",
    "    parser.add_argument('--num_gc_layers', type=int)\n",
    "    parser.add_argument('--hidden_dim', type=int)\n",
    "    parser.add_argument('--epoch', type=int)\n",
    "    parser.add_argument('--batch', type=int)\n",
    "    parser.add_argument('--aug', type=str)\n",
    "    parser.add_argument('--tau', type=float)\n",
    "    parser.add_argument('--drop_ratio', type=float)\n",
    "    parser.add_argument('--num_latent_factors', type=int)\n",
    "    parser.add_argument('--head_layers', type=int)\n",
    "    parser.add_argument('--JK', type=str, choices=['last', 'sum'])\n",
    "    parser.add_argument('--residual', type=int, choices=[0, 1])\n",
    "    parser.add_argument('--proj', type=int, choices=[0, 1])\n",
    "    parser.add_argument('--pool', type=str, choices=['mean', 'sum', 'max'])\n",
    "    parser.add_argument('--fe', type=int)\n",
    "    parser.add_argument('--seed', type=int)\n",
    "    parser.add_argument('--num_workers', type=int)\n",
    "    parser.add_argument('--log_dir', type=str)\n",
    "    parser.add_argument('--log_interval', type=int, default=5)\n",
    "    parser.add_argument(\"--debug\", action='store_true', default=False)\n",
    "\n",
    "    args, unknown = parser.parse_known_args([])\n",
    "\n",
    "    with open(f'config/{DS}.yml', 'r') as f:\n",
    "        config_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    for k, v in vars(args).items():\n",
    "        if v is not None:\n",
    "            config_yaml[k] = v\n",
    "    config_ns = argparse.Namespace(**config_yaml)\n",
    "\n",
    "    return config_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_parse()\n",
    "setup_seed(args.seed)\n",
    "\n",
    "accuracies = {'result': [], 'result_val': []}\n",
    "epochs = args.epoch\n",
    "log_interval = args.log_interval\n",
    "batch_size = args.batch\n",
    "lr = args.lr\n",
    "DS = args.DS\n",
    "args.hidden_dim = int(args.hidden_dim // args.num_latent_factors) * args.num_latent_factors\n",
    "path = './data'\n",
    "dataset = TUDataset_aug(root=path, name=DS, aug=args.aug).shuffle()\n",
    "dataset_eval = TUDataset_aug(root=path, name=DS, aug='none').shuffle()\n",
    "print(len(dataset))\n",
    "print(dataset.get_num_feature())\n",
    "try:\n",
    "    dataset_num_features = dataset.get_num_feature()\n",
    "except:\n",
    "    dataset_num_features = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=args.num_workers)\n",
    "dataloader_eval = DataLoader(dataset_eval, batch_size=batch_size, num_workers=args.num_workers)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCL(\n",
    "    num_features=dataset_num_features,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    num_layer=args.num_gc_layers,\n",
    "    device=device,\n",
    "    args=args\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.eval()\n",
    "emb, y = model.encoder.get_embeddings(dataloader_eval)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_all = 0\n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        data, data_aug = data\n",
    "        optimizer.zero_grad()\n",
    "        node_num, _ = data.x.size()\n",
    "        data = data.to(device)\n",
    "        x = model(data.x, data.edge_index, data.batch, data.num_graphs)\n",
    "        edge_idx = data_aug.edge_index.numpy()\n",
    "        _, edge_num = edge_idx.shape\n",
    "        idx_not_missing = [n for n in range(node_num) if (n in edge_idx[0] or n in edge_idx[1])]\n",
    "        node_num_aug = len(idx_not_missing)\n",
    "        data_aug.x = data_aug.x[idx_not_missing]\n",
    "        data_aug.batch = data.batch[idx_not_missing]\n",
    "        idx_dict = {idx_not_missing[n]: n for n in range(node_num_aug)}\n",
    "        edge_idx = [[idx_dict[edge_idx[0, n]], idx_dict[edge_idx[1, n]]] for n in range(edge_num) if\n",
    "                    not edge_idx[0, n] == edge_idx[1, n]]\n",
    "        data_aug.edge_index = torch.tensor(edge_idx).transpose_(0, 1)\n",
    "        data_aug = data_aug.to(device)\n",
    "        x_aug = model(data_aug.x, data_aug.edge_index, data_aug.batch, data_aug.num_graphs)\n",
    "        loss = model.loss_cal(x, x_aug)\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('loss %.4f' % (loss_all / len(dataloader)))\n",
    "    if log_interval > 0 and epoch % log_interval == 0:\n",
    "        model.eval()\n",
    "        emb, y = model.encoder.get_embeddings(dataloader_eval)\n",
    "        result, result_val = evaluate_embedding(emb, y)\n",
    "        accuracies['result'].append(result)\n",
    "        accuracies['result_val'].append(result_val)\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_parse('HIV')\n",
    "setup_seed(args.seed)\n",
    "\n",
    "accuracies = {'result': [], 'result_val': []}\n",
    "epochs = args.epoch\n",
    "log_interval = args.log_interval\n",
    "batch_size = args.batch\n",
    "lr = args.lr\n",
    "DS = args.DS\n",
    "args.hidden_dim = int(args.hidden_dim // args.num_latent_factors) * args.num_latent_factors\n",
    "path = '../data'\n",
    "dataset = OGBDataset_aug(root=path, name=\"ogbg-molhiv\", aug=args.aug).shuffle()\n",
    "dataset_eval = OGBDataset_aug(root=path, name=\"ogbg-molhiv\", aug='none').shuffle()\n",
    "print(len(dataset))\n",
    "print(dataset.get_num_feature())\n",
    "try:\n",
    "    dataset_num_features = dataset.get_num_feature()\n",
    "except:\n",
    "    dataset_num_features = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=args.num_workers)\n",
    "dataloader_eval = DataLoader(dataset_eval, batch_size=batch_size, num_workers=args.num_workers)\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCL(\n",
    "    num_features=dataset_num_features,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    num_layer=args.num_gc_layers,\n",
    "    device=device,\n",
    "    args=args\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# model.eval()\n",
    "# emb, y = model.encoder.get_embeddings(dataloader_eval)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_all = 0\n",
    "    model.train()\n",
    "    for data in dataloader:\n",
    "        data, data_aug = data\n",
    "        optimizer.zero_grad()\n",
    "        node_num, _ = data.x.size()\n",
    "        data.x = data.x.float()\n",
    "        data = data.to(device)\n",
    "        x = model(data.x, data.edge_index, data.batch, data.num_graphs)\n",
    "        edge_idx = data_aug.edge_index.numpy()\n",
    "        _, edge_num = edge_idx.shape\n",
    "        idx_not_missing = [n for n in range(node_num) if (n in edge_idx[0] or n in edge_idx[1])]\n",
    "        node_num_aug = len(idx_not_missing)\n",
    "        data_aug.x = data_aug.x[idx_not_missing]\n",
    "        data_aug.batch = data.batch[idx_not_missing]\n",
    "        idx_dict = {idx_not_missing[n]: n for n in range(node_num_aug)}\n",
    "        edge_idx = [[idx_dict[edge_idx[0, n]], idx_dict[edge_idx[1, n]]] for n in range(edge_num) if\n",
    "                    not edge_idx[0, n] == edge_idx[1, n]]\n",
    "        data_aug.edge_index = torch.tensor(edge_idx).transpose_(0, 1)\n",
    "        data_aug.x = data_aug.x.float()\n",
    "        data_aug = data_aug.to(device)\n",
    "        x_aug = model(data_aug.x, data_aug.edge_index, data_aug.batch, data_aug.num_graphs)\n",
    "        loss = model.loss_cal(x, x_aug)\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('loss %.4f' % (loss_all / len(dataloader)))\n",
    "    if log_interval > 0 and epoch % log_interval == 0:\n",
    "        model.eval()\n",
    "        emb, y = model.encoder.get_embeddings(dataloader_eval)\n",
    "        result, result_val = evaluate_embedding(emb, y)\n",
    "        accuracies['result'].append(result)\n",
    "        accuracies['result_val'].append(result_val)\n",
    "\n",
    "with open(\"../result/acc\", \"a+\") as f:\n",
    "    f.write(accuracies + \"\\n\")\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
